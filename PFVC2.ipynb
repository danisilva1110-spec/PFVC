{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5EYs4JJJQG6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f9ecad-3dca-422f-e89d-f48fc41a4e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PFVC\n",
            "classes_trashnet.json\t      PFVC1.ipynb  README.md\n",
            "modelo_resnet18_trashnet.pth  PFVC2.ipynb  UAVVaste\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0. CLONAR O REPOSITÓRIO DO PROJETO E ENTRAR NA PASTA\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/danisilva1110-spec/PFVC.git\"\n",
        "REPO_NAME = \"PFVC\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "%cd {REPO_NAME}\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. CLONAR O DATASET UAVVaste DENTRO DO PROJETO E BAIXAR OS DADOS\n",
        "# ============================================================\n",
        "\n",
        "UAVVASTE_URL = \"https://github.com/PUTvision/UAVVaste.git\"\n",
        "UAVVASTE_DIR = \"UAVVaste\"\n",
        "\n",
        "# Clona o repositório se ainda não existir\n",
        "if not os.path.exists(UAVVASTE_DIR):\n",
        "    !git clone {UAVVASTE_URL}\n",
        "\n",
        "# Entra na pasta do UAVVaste\n",
        "%cd {UAVVASTE_DIR}\n",
        "!ls\n",
        "\n",
        "# (Opcional, mas bom)\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Roda o script que baixa o dataset (imagens + anotações)\n",
        "!python main.py\n",
        "\n",
        "# Volta para a pasta PFVC\n",
        "%cd .."
      ],
      "metadata": {
        "id": "ludkGvvuQPN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db24abbc-6912-4a19-ceb2-29cb3eee0cef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PFVC/UAVVaste\n",
            "annotations  images  LICENSE  main.py  README.md  requirements.txt  tools\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 1)) (2025.11.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools->-r requirements.txt (line 2)) (2.0.2)\n",
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "100% 772/772 [06:41<00:00,  1.92it/s]\n",
            "/content/PFVC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2. CAMINHOS IMPORTANTES\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "BASE_DIR = os.getcwd()                 # /content/PFVC\n",
        "UAVVASTE_DIR = os.path.join(BASE_DIR, \"UAVVaste\")\n",
        "UAV_IMAGES_DIR = os.path.join(UAVVASTE_DIR, \"images\")\n",
        "UAV_ANNO_DIR   = os.path.join(UAVVASTE_DIR, \"annotations\")\n",
        "\n",
        "print(\"PFVC base:\", BASE_DIR)\n",
        "print(\"UAVVaste:\", UAVVASTE_DIR)\n",
        "print(\"Imagens :\", UAV_IMAGES_DIR)\n",
        "print(\"Annots  :\", UAV_ANNO_DIR)\n",
        "\n",
        "print(\"Conteúdo de annotations:\", os.listdir(UAV_ANNO_DIR))"
      ],
      "metadata": {
        "id": "SaP4h_aZQQi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cccf4e4-5d0d-4943-d010-65f5c6b3aa03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PFVC base: /content/PFVC\n",
            "UAVVaste: /content/PFVC/UAVVaste\n",
            "Imagens : /content/PFVC/UAVVaste/images\n",
            "Annots  : /content/PFVC/UAVVaste/annotations\n",
            "Conteúdo de annotations: ['annotations.json', 'train_val_test_distribution_file.json', 'flickurls.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3. LER ANOTAÇÕES COCO (PROCURAR AUTOMATICAMENTE O .JSON)\n",
        "# ============================================================\n",
        "import glob\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "pattern = os.path.join(UAV_ANNO_DIR, \"*.json\")\n",
        "json_files = glob.glob(pattern)\n",
        "\n",
        "if not json_files:\n",
        "    raise FileNotFoundError(f\"Nenhum arquivo .json encontrado em {UAV_ANNO_DIR}\")\n",
        "\n",
        "ann_file = json_files[0]  # pega o primeiro .json encontrado\n",
        "print(\"Usando arquivo de anotações:\", ann_file)\n",
        "\n",
        "with open(ann_file, \"r\") as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "print(\"Chaves do COCO:\", coco.keys())\n",
        "print(\"Total de imagens:\", len(coco[\"images\"]))\n",
        "print(\"Total de anotações:\", len(coco[\"annotations\"]))\n",
        "\n",
        "# ---------- AQUI vêm as variáveis que sua função usa ----------\n",
        "# dicionário: id_da_imagem -> info da imagem\n",
        "images_info = {img[\"id\"]: img for img in coco[\"images\"]}\n",
        "\n",
        "# dicionário: id_da_imagem -> lista de anotações\n",
        "anns_by_img = defaultdict(list)\n",
        "for ann in coco[\"annotations\"]:\n",
        "    anns_by_img[ann[\"image_id\"]].append(ann)\n",
        "\n",
        "print(\"Imagens indexadas:\", len(images_info))\n",
        "print(\"Imagens com anotações:\", len(anns_by_img))"
      ],
      "metadata": {
        "id": "4ebt0wJzRNjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d8c44d-50e5-434a-c4d5-7af2a8db9560"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando arquivo de anotações: /content/PFVC/UAVVaste/annotations/annotations.json\n",
            "Chaves do COCO: dict_keys(['images', 'categories', 'annotations', 'licenses', 'info'])\n",
            "Total de imagens: 772\n",
            "Total de anotações: 3718\n",
            "Imagens indexadas: 772\n",
            "Imagens com anotações: 772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4. CRIAR ESTRUTURA PARA DATASET BINÁRIO (FOCOS)\n",
        "# ============================================================\n",
        "FOCOS_ROOT = os.path.join(BASE_DIR, \"data_focos\")\n",
        "LIXO_DIR = os.path.join(FOCOS_ROOT, \"lixo\")\n",
        "NAO_LIXO_DIR = os.path.join(FOCOS_ROOT, \"nao_lixo\")\n",
        "\n",
        "os.makedirs(LIXO_DIR, exist_ok=True)\n",
        "os.makedirs(NAO_LIXO_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Diretório base dos focos:\", FOCOS_ROOT)\n",
        "print(\"  ->\", LIXO_DIR)\n",
        "print(\"  ->\", NAO_LIXO_DIR)\n"
      ],
      "metadata": {
        "id": "D6V8vqTYRTJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e914a9a-8d8b-4f44-e85d-954fd45a94fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório base dos focos: /content/PFVC/data_focos\n",
            "  -> /content/PFVC/data_focos/lixo\n",
            "  -> /content/PFVC/data_focos/nao_lixo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5. GERAR PATCHES \"LIXO\" A PARTIR DAS CAIXAS ANOTADAS\n",
        "# ============================================================\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def gerar_patches_lixo(margin=0.2, max_imgs=None):\n",
        "    \"\"\"\n",
        "    margin: porcentagem para aumentar a caixa (0.2 = 20%)\n",
        "    max_imgs: limita quantas imagens usar (None = todas)\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    img_ids = list(images_info.keys())\n",
        "\n",
        "    if max_imgs is not None:\n",
        "        img_ids = img_ids[:max_imgs]\n",
        "\n",
        "    for idx, img_id in enumerate(img_ids):\n",
        "        img_info = images_info[img_id]\n",
        "        file_name = img_info[\"file_name\"]\n",
        "        img_path = os.path.join(UAV_IMAGES_DIR, file_name)\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        anns = anns_by_img.get(img_id, [])\n",
        "\n",
        "        for ann in anns:\n",
        "            x, y, bw, bh = ann[\"bbox\"]  # COCO: [x, y, width, height]\n",
        "            x, y, bw, bh = float(x), float(y), float(bw), float(bh)\n",
        "\n",
        "            # centro da bbox\n",
        "            cx = x + bw / 2.0\n",
        "            cy = y + bh / 2.0\n",
        "\n",
        "            # aumenta bbox\n",
        "            bw2 = bw * (1.0 + margin)\n",
        "            bh2 = bh * (1.0 + margin)\n",
        "\n",
        "            x1 = int(max(0, cx - bw2 / 2.0))\n",
        "            y1 = int(max(0, cy - bh2 / 2.0))\n",
        "            x2 = int(min(w, cx + bw2 / 2.0))\n",
        "            y2 = int(min(h, cy + bh2 / 2.0))\n",
        "\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                continue\n",
        "\n",
        "            patch = img[y1:y2, x1:x2]\n",
        "            if patch.size == 0:\n",
        "                continue\n",
        "\n",
        "            out_name = f\"lixo_{img_id}_{ann['id']}.jpg\"\n",
        "            out_path = os.path.join(LIXO_DIR, out_name)\n",
        "            cv2.imwrite(out_path, patch)\n",
        "            count += 1\n",
        "\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            print(f\"Processadas {idx+1} imagens...\")\n",
        "\n",
        "    print(\"Total de patches de LIXO gerados:\", count)\n",
        "\n",
        "# RODAR (ajuste max_imgs se quiser menos imagens na primeira vez)\n",
        "gerar_patches_lixo(margin=0.2, max_imgs=200)  # por exemplo, 200 imagens\n"
      ],
      "metadata": {
        "id": "grXEY4UeRUbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e643f57-8cad-473c-a258-846bea1f3209"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processadas 50 imagens...\n",
            "Processadas 100 imagens...\n",
            "Processadas 150 imagens...\n",
            "Processadas 200 imagens...\n",
            "Total de patches de LIXO gerados: 1117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6. GERAR PATCHES \"NAO_LIXO\" (FUNDOS) ALEATÓRIOS\n",
        "# ============================================================\n",
        "import random\n",
        "\n",
        "def ponto_dentro_bbox(bbox, px, py):\n",
        "    x, y, bw, bh = bbox\n",
        "    return (px >= x) and (px <= x + bw) and (py >= y) and (py <= y + bh)\n",
        "\n",
        "def gerar_patches_nao_lixo(patch_size=224, samples_per_img=5, max_imgs=None):\n",
        "    \"\"\"\n",
        "    patch_size: tamanho do patch quadrado em pixels\n",
        "    samples_per_img: quantos patches de fundo tentar por imagem\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    img_ids = list(images_info.keys())\n",
        "\n",
        "    if max_imgs is not None:\n",
        "        img_ids = img_ids[:max_imgs]\n",
        "\n",
        "    for idx, img_id in enumerate(img_ids):\n",
        "        img_info = images_info[img_id]\n",
        "        file_name = img_info[\"file_name\"]\n",
        "        img_path = os.path.join(UAV_IMAGES_DIR, file_name)\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        bboxes = [ann[\"bbox\"] for ann in anns_by_img.get(img_id, [])]\n",
        "\n",
        "        # se a imagem for menor que o patch, pula\n",
        "        if h <= patch_size or w <= patch_size:\n",
        "            continue\n",
        "\n",
        "        gerados_img = 0\n",
        "        tentativas = 0\n",
        "        max_tentativas = samples_per_img * 10\n",
        "\n",
        "        while gerados_img < samples_per_img and tentativas < max_tentativas:\n",
        "            tentativas += 1\n",
        "\n",
        "            x1 = random.randint(0, w - patch_size)\n",
        "            y1 = random.randint(0, h - patch_size)\n",
        "            x2 = x1 + patch_size\n",
        "            y2 = y1 + patch_size\n",
        "\n",
        "            cx = x1 + patch_size / 2.0\n",
        "            cy = y1 + patch_size / 2.0\n",
        "\n",
        "            # verifica se o centro cai dentro de alguma bbox\n",
        "            inside_any = False\n",
        "            for bbox in bboxes:\n",
        "                if ponto_dentro_bbox(bbox, cx, cy):\n",
        "                    inside_any = True\n",
        "                    break\n",
        "\n",
        "            if inside_any:\n",
        "                continue\n",
        "\n",
        "            patch = img[y1:y2, x1:x2]\n",
        "            if patch.size == 0:\n",
        "                continue\n",
        "\n",
        "            out_name = f\"bg_{img_id}_{gerados_img}.jpg\"\n",
        "            out_path = os.path.join(NAO_LIXO_DIR, out_name)\n",
        "            cv2.imwrite(out_path, patch)\n",
        "\n",
        "            gerados_img += 1\n",
        "            count += 1\n",
        "\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            print(f\"Processadas {idx+1} imagens...\")\n",
        "\n",
        "    print(\"Total de patches de NAO_LIXO gerados:\", count)\n",
        "\n",
        "# RODAR\n",
        "gerar_patches_nao_lixo(patch_size=224, samples_per_img=5, max_imgs=200)\n"
      ],
      "metadata": {
        "id": "RUin52VVRWXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea4eb8f-d743-475a-83ea-8a4ce28ee09a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processadas 50 imagens...\n",
            "Processadas 100 imagens...\n",
            "Processadas 150 imagens...\n",
            "Processadas 200 imagens...\n",
            "Total de patches de NAO_LIXO gerados: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. IMPORTS PARA TREINO DO MODELO A (FOCOS)\n",
        "# ============================================================\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device para treino do modelo de focos:\", device)"
      ],
      "metadata": {
        "id": "JZYd_n_SeVY5",
        "outputId": "d3554e3e-99cb-475f-dd31-072ad62c84ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device para treino do modelo de focos: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8. DATASET E DATALOADERS PARA FOCOS (lixo / nao_lixo)\n",
        "# ============================================================\n",
        "\n",
        "# raiz onde ficam as pastas lixo/ e nao_lixo\n",
        "focos_dir = FOCOS_ROOT  # já definido antes como .../data_focos\n",
        "\n",
        "input_size = 224\n",
        "\n",
        "train_transform_foco = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform_foco = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# ImageFolder assume subpastas = classes\n",
        "full_dataset_foco = datasets.ImageFolder(root=focos_dir,\n",
        "                                         transform=train_transform_foco)\n",
        "\n",
        "class_names_foco = full_dataset_foco.classes\n",
        "print(\"Classes do modelo de focos:\", class_names_foco)\n",
        "num_classes_foco = len(class_names_foco)\n",
        "\n",
        "# split 80/20 para treino/validação\n",
        "num_total = len(full_dataset_foco)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_val = num_total - num_train\n",
        "\n",
        "train_dataset_foco, val_dataset_foco = random_split(full_dataset_foco,\n",
        "                                                    [num_train, num_val])\n",
        "\n",
        "# validação usa transform diferente (sem augmentação)\n",
        "val_dataset_foco.dataset.transform = val_transform_foco\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader_foco = DataLoader(train_dataset_foco,\n",
        "                               batch_size=batch_size,\n",
        "                               shuffle=True,\n",
        "                               num_workers=2)\n",
        "\n",
        "val_loader_foco = DataLoader(val_dataset_foco,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=False,\n",
        "                             num_workers=2)\n",
        "\n",
        "dataloaders_foco = {\"train\": train_loader_foco,\n",
        "                    \"val\": val_loader_foco}\n",
        "\n",
        "dataset_sizes_foco = {\n",
        "    \"train\": len(train_dataset_foco),\n",
        "    \"val\": len(val_dataset_foco)\n",
        "}\n",
        "\n",
        "print(\"Tamanho do dataset de focos:\", dataset_sizes_foco)\n"
      ],
      "metadata": {
        "id": "_x_6sA94eWNG",
        "outputId": "5575c552-a5c5-4883-85d1-35fdd5a74638",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes do modelo de focos: ['lixo', 'nao_lixo']\n",
            "Tamanho do dataset de focos: {'train': 1693, 'val': 424}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9. DEFINIR MODELO RESNET18 PARA FOCOS (BINÁRIO)\n",
        "# ============================================================\n",
        "\n",
        "modelo_foco = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# congela todas as camadas\n",
        "for param in modelo_foco.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# substitui a última camada para 2 classes (ou quantas tiverem em data_focos)\n",
        "in_features = modelo_foco.fc.in_features\n",
        "modelo_foco.fc = nn.Linear(in_features, num_classes_foco)\n",
        "\n",
        "modelo_foco = modelo_foco.to(device)\n",
        "print(modelo_foco.fc)\n"
      ],
      "metadata": {
        "id": "waI6Ioo3ecOZ",
        "outputId": "062cb6ea-9ffb-4dc1-f045-f3f124d42f89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=512, out_features=2, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 10. FUNÇÃO DE TREINO PARA O MODELO DE FOCOS\n",
        "# ============================================================\n",
        "\n",
        "criterion_foco = nn.CrossEntropyLoss()\n",
        "optimizer_foco = optim.Adam(modelo_foco.fc.parameters(), lr=1e-4)\n",
        "\n",
        "def train_model_foco(model, dataloaders, dataset_sizes,\n",
        "                     criterion, optimizer, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc  = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            # guarda melhor modelo na validação\n",
        "            if phase == \"val\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Treino completo em {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")\n",
        "    print(f\"Melhor Acc val: {best_acc:.4f}\")\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "47ApzdlZedwC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 11. TREINAR O MODELO DE FOCOS\n",
        "# ============================================================\n",
        "\n",
        "num_epochs = 10  # pode ajustar depois\n",
        "modelo_foco = train_model_foco(\n",
        "    modelo_foco,\n",
        "    dataloaders_foco,\n",
        "    dataset_sizes_foco,\n",
        "    criterion_foco,\n",
        "    optimizer_foco,\n",
        "    num_epochs=num_epochs\n",
        ")\n"
      ],
      "metadata": {
        "id": "x-1G5j6Yefel",
        "outputId": "425a25ee-9865-4ec2-c87a-830218d8d9ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "------------------------------\n",
            "train Loss: 0.6989 Acc: 0.5458\n",
            "val Loss: 0.6011 Acc: 0.6533\n",
            "\n",
            "Epoch 2/10\n",
            "------------------------------\n",
            "train Loss: 0.5374 Acc: 0.7626\n",
            "val Loss: 0.4792 Acc: 0.8255\n",
            "\n",
            "Epoch 3/10\n",
            "------------------------------\n",
            "train Loss: 0.4372 Acc: 0.8458\n",
            "val Loss: 0.3963 Acc: 0.8679\n",
            "\n",
            "Epoch 4/10\n",
            "------------------------------\n",
            "train Loss: 0.3668 Acc: 0.8890\n",
            "val Loss: 0.3504 Acc: 0.8962\n",
            "\n",
            "Epoch 5/10\n",
            "------------------------------\n",
            "train Loss: 0.3225 Acc: 0.9002\n",
            "val Loss: 0.3095 Acc: 0.9057\n",
            "\n",
            "Epoch 6/10\n",
            "------------------------------\n",
            "train Loss: 0.2927 Acc: 0.9114\n",
            "val Loss: 0.2860 Acc: 0.9127\n",
            "\n",
            "Epoch 7/10\n",
            "------------------------------\n",
            "train Loss: 0.2734 Acc: 0.9167\n",
            "val Loss: 0.2688 Acc: 0.9080\n",
            "\n",
            "Epoch 8/10\n",
            "------------------------------\n",
            "train Loss: 0.2492 Acc: 0.9238\n",
            "val Loss: 0.2594 Acc: 0.9175\n",
            "\n",
            "Epoch 9/10\n",
            "------------------------------\n",
            "train Loss: 0.2473 Acc: 0.9209\n",
            "val Loss: 0.2455 Acc: 0.9175\n",
            "\n",
            "Epoch 10/10\n",
            "------------------------------\n",
            "train Loss: 0.2373 Acc: 0.9173\n",
            "val Loss: 0.2367 Acc: 0.9127\n",
            "\n",
            "Treino completo em 0m 26s\n",
            "Melhor Acc val: 0.9175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 12. SALVAR MODELO DE FOCOS E CLASSES\n",
        "# ============================================================\n",
        "import json\n",
        "\n",
        "FOCO_MODEL_PATH = \"modelo_foco_lixo.pth\"\n",
        "FOCO_CLASSES_PATH = \"classes_foco.json\"\n",
        "\n",
        "torch.save(modelo_foco.state_dict(), FOCO_MODEL_PATH)\n",
        "with open(FOCO_CLASSES_PATH, \"w\") as f:\n",
        "    json.dump(class_names_foco, f)\n",
        "\n",
        "print(\"Modelo de focos salvo em:\", FOCO_MODEL_PATH)\n",
        "print(\"Classes de focos salvas em:\", FOCO_CLASSES_PATH)\n"
      ],
      "metadata": {
        "id": "y0pYTNcbeiDg",
        "outputId": "10988ff0-ff01-45c5-fedf-aee5540af6a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de focos salvo em: modelo_foco_lixo.pth\n",
            "Classes de focos salvas em: classes_foco.json\n"
          ]
        }
      ]
    }
  ]
}